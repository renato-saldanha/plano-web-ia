# üìö Guia Completo: LangChain B√°sico

Este guia fornece uma introdu√ß√£o detalhada ao LangChain, o framework mais popular para criar aplica√ß√µes com LLMs (Large Language Models).

---

## üìã √çndice

1. [O que √© LangChain?](#-o-que-√©-langchain)
2. [Por que usar LangChain?](#-por-que-usar-langchain)
3. [Instala√ß√£o e Setup](#-instala√ß√£o-e-setup)
4. [Conceitos B√°sicos](#-conceitos-b√°sicos)
5. [Primeiro Exemplo](#-primeiro-exemplo)
6. [Compara√ß√£o com C√≥digo Manual](#-compara√ß√£o-com-c√≥digo-manual)
7. [Pr√≥ximos Passos](#-pr√≥ximos-passos)

---

## üéØ O que √© LangChain?

**LangChain** √© um framework Python de c√≥digo aberto criado para facilitar o desenvolvimento de aplica√ß√µes com LLMs (Large Language Models).

### Defini√ß√£o Simples

Pense no LangChain como uma **camada de abstra√ß√£o** que simplifica o uso de LLMs. Em vez de escrever c√≥digo manual para cada API diferente (Groq, OpenAI, Gemini, Claude), voc√™ usa uma interface unificada.

### Analogia

Imagine que voc√™ precisa dirigir carros diferentes:
- **Sem LangChain:** Precisa aprender como ligar, acelerar e frear cada modelo espec√≠fico
- **Com LangChain:** Todos os carros t√™m os mesmos controles b√°sicos, voc√™ s√≥ muda o "motor" (LLM)

---

## üí° Por que usar LangChain?

### Problemas do C√≥digo Manual (Semana 1)

Na Semana 1, aprendemos a usar APIs diretamente. Isso funcionou, mas:

1. **C√≥digo Repetitivo**
   ```python
   # Sempre precisa criar cliente, fazer chamada, tratar resposta
   client = Groq(api_key=api_key)
   response = client.chat.completions.create(...)
   result = response.choices[0].message.content
   ```

2. **Dif√≠cil Trocar LLMs**
   - Cada API tem sintaxe diferente
   - Precisa reescrever c√≥digo para cada LLM
   - Dif√≠cil comparar resultados

3. **Sem Padr√µes**
   - Cada desenvolvedor faz diferente
   - Dif√≠cil manter e escalar
   - N√£o aproveita padr√µes da ind√∫stria

### Vantagens do LangChain

1. **‚úÖ Menos C√≥digo**
   - Reduz boilerplate significativamente
   - C√≥digo mais limpo e leg√≠vel

2. **‚úÖ Trocar LLMs Facilmente**
   - Mesma interface para todos os LLMs
   - Trocar de Groq para Gemini = mudar 1 linha

3. **‚úÖ Padr√£o da Ind√∫stria**
   - Framework mais usado
   - Comunidade grande e ativa
   - Documenta√ß√£o excelente

4. **‚úÖ Funcionalidades Avan√ßadas**
   - Chains (sequ√™ncias de opera√ß√µes)
   - RAG (Retrieval-Augmented Generation)
   - Agents (agentes aut√¥nomos)
   - Memory (mem√≥ria entre conversas)

---

## üîß Instala√ß√£o e Setup

### Passo 1: Instalar LangChain

```bash
# Ativar ambiente virtual primeiro
# Windows
venv\Scripts\activate

# Mac/Linux
source venv/bin/activate

# Instalar LangChain e integra√ß√µes
pip install langchain langchain-groq langchain-google-genai langchain-anthropic
```

### Passo 2: Verificar Instala√ß√£o

```bash
python -c "import langchain; print(langchain.__version__)"
```

Deve mostrar a vers√£o instalada (ex: `0.1.0`).

### Passo 3: Configurar Vari√°veis de Ambiente

Certifique-se de que seu arquivo `.env` (na raiz do projeto) cont√©m:

```env
GROQ_API_KEY=sua_chave_groq_aqui
GEMINI_API_KEY=sua_chave_gemini_aqui
ANTHROPIC_API_KEY=sua_chave_anthropic_aqui
```

**Nota:** Voc√™ j√° configurou isso na Semana 1! ‚úÖ

---

## üìö Conceitos B√°sicos

### 1. LLM (Large Language Model)

**O que √©:** Um modelo de linguagem (Groq, Gemini, Claude, GPT-4, etc.)

**No LangChain:** Representado por classes como `ChatGroq`, `ChatGoogleGenerativeAI`, `ChatAnthropic`

**Exemplo:**
```python
from langchain_groq import ChatGroq

llm = ChatGroq(model="llama-3.1-8b-instant")
```

### 2. Prompt

**O que √©:** A entrada (pergunta/texto) que voc√™ envia para o LLM

**No LangChain:** Pode ser string simples ou `PromptTemplate` (mais avan√ßado)

**Exemplo:**
```python
prompt = "Explique o que √© Python em 2 frases"
```

### 3. Chain

**O que √©:** Sequ√™ncia de opera√ß√µes conectadas

**No LangChain:** Conecta m√∫ltiplas opera√ß√µes (prompt ‚Üí LLM ‚Üí processamento ‚Üí resposta)

**Exemplo:**
```python
# Chain simples: Prompt ‚Üí LLM ‚Üí Resposta
chain = prompt | llm
```

**Nota:** Chains ser√£o exploradas em detalhes no Dia 2.

### 4. Message

**O que √©:** Mensagem estruturada para conversas

**No LangChain:** `HumanMessage` (usu√°rio), `AIMessage` (assistente), `SystemMessage` (sistema)

**Exemplo:**
```python
from langchain_core.messages import HumanMessage

message = HumanMessage(content="Ol√°!")
```

---

## üöÄ Primeiro Exemplo

Vamos criar um exemplo simples comparando c√≥digo manual vs LangChain.

### C√≥digo Manual (Semana 1)

```python
# hello_ai_groq.py (Semana 1)
import os
from dotenv import load_dotenv
from groq import Groq

load_dotenv()
api_key = os.getenv("GROQ_API_KEY")
client = Groq(api_key=api_key)

response = client.chat.completions.create(
    messages=[{"role": "user", "content": "Ol√°! Me apresente."}],
    model="llama-3.1-8b-instant"
)

print(response.choices[0].message.content)
```

**Linhas de c√≥digo:** ~10 linhas  
**Complexidade:** M√©dia (precisa entender estrutura da API)

### C√≥digo com LangChain

```python
# exemplo_langchain_basico.py
import os
from dotenv import load_dotenv
from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage

load_dotenv()

# Criar LLM
llm = ChatGroq(
    model="llama-3.1-8b-instant",
    temperature=0.7
)

# Criar mensagem
message = HumanMessage(content="Ol√°! Me apresente.")

# Invocar LLM
response = llm.invoke([message])

print(response.content)
```

**Linhas de c√≥digo:** ~15 linhas (similar)  
**Complexidade:** Baixa (mais intuitivo)  
**Vantagem:** Mesma sintaxe funciona para qualquer LLM!

### Trocar LLM (Vantagem Real)

**C√≥digo Manual:** Precisa reescrever tudo
```python
# Trocar para Gemini = c√≥digo completamente diferente
from google import generativeai
genai.configure(api_key=api_key)
model = genai.GenerativeModel('gemini-pro')
response = model.generate_content("Ol√°!")
```

**LangChain:** Trocar 1 linha
```python
# Trocar para Gemini = mudar apenas a classe
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(model="gemini-pro")  # S√≥ mudou esta linha!
response = llm.invoke([message])  # Resto igual!
```

---

## üîç Compara√ß√£o com C√≥digo Manual

### Tabela Comparativa

| Aspecto | C√≥digo Manual | LangChain |
|---------|---------------|-----------|
| **Linhas de c√≥digo** | ~10-15 | ~10-15 |
| **Legibilidade** | M√©dia | Alta |
| **Trocar LLM** | Dif√≠cil (reescrever) | F√°cil (1 linha) |
| **Padr√µes** | Cada um faz diferente | Padr√£o da ind√∫stria |
| **Funcionalidades avan√ßadas** | Implementar manualmente | J√° inclu√≠das (Chains, RAG, Agents) |
| **Curva de aprendizado** | M√©dia | Baixa (ap√≥s setup inicial) |

### Quando Usar Cada Abordagem?

**Use C√≥digo Manual quando:**
- Projeto muito simples (1-2 chamadas)
- Precisa de controle total sobre requisi√ß√µes
- N√£o quer depend√™ncias extras

**Use LangChain quando:**
- Projeto vai crescer
- Precisa trocar LLMs facilmente
- Quer usar Chains, RAG, Agents
- Quer seguir padr√µes da ind√∫stria

---

## üìñ Estrutura de um Script LangChain

### Padr√£o B√°sico

```python
# 1. Imports
from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage
from dotenv import load_dotenv
import os

# 2. Configura√ß√£o
load_dotenv()

# 3. Criar LLM
llm = ChatGroq(
    model="llama-3.1-8b-instant",
    temperature=0.7
)

# 4. Criar mensagem
message = HumanMessage(content="Seu prompt aqui")

# 5. Invocar LLM
response = llm.invoke([message])

# 6. Processar resposta
print(response.content)
```

### Explica√ß√£o de Cada Parte

1. **Imports:** Importar classes necess√°rias
2. **Configura√ß√£o:** Carregar vari√°veis de ambiente
3. **Criar LLM:** Instanciar o modelo desejado
4. **Criar mensagem:** Preparar entrada para o LLM
5. **Invocar:** Enviar mensagem e receber resposta
6. **Processar:** Usar resposta conforme necess√°rio

---

## üéØ Par√¢metros Comuns

### Temperature

**O que √©:** Controla criatividade/aleatoriedade (0.0 a 1.0)

**Valores:**
- `0.0`: Determin√≠stico, sempre mesma resposta
- `0.7`: Balanceado (padr√£o recomendado)
- `1.0`: Muito criativo, respostas variadas

**Exemplo:**
```python
llm = ChatGroq(temperature=0.7)  # Balanceado
```

### Max Tokens

**O que √©:** Limite m√°ximo de tokens na resposta

**Exemplo:**
```python
llm = ChatGroq(max_tokens=500)  # M√°ximo 500 tokens
```

### Model

**O que √©:** Qual modelo usar (depende do LLM)

**Exemplos:**
```python
# Groq
llm = ChatGroq(model="llama-3.1-8b-instant")

# Gemini
llm = ChatGoogleGenerativeAI(model="gemini-pro")

# Claude
llm = ChatAnthropic(model="claude-3-sonnet-20240229")
```

---

## üîÑ Pr√≥ximos Passos

Agora que voc√™ entendeu o b√°sico:

1. **Execute o exemplo:** `exemplo_langchain_basico.py`
2. **Complete os exerc√≠cios:** `exercicios_langchain.md`
3. **Compare com c√≥digo manual:** Veja diferen√ßas pr√°ticas
4. **Dia 2:** Aprender Chains e sequ√™ncias

---

## üìö Recursos Adicionais

### Documenta√ß√£o Oficial
- [LangChain Docs](https://python.langchain.com/)
- [LangChain Quickstart](https://python.langchain.com/docs/get_started/introduction)
- [LangChain LLMs](https://python.langchain.com/docs/integrations/llms/)

### Tutoriais
- [LangChain YouTube](https://www.youtube.com/@LangChain)
- [LangChain Tutorials](https://python.langchain.com/docs/tutorials)

### Comunidade
- [LangChain Discord](https://discord.gg/langchain)
- [LangChain GitHub](https://github.com/langchain-ai/langchain)

---

## ‚ùì Perguntas Frequentes

### Preciso aprender tudo de uma vez?

**N√£o!** Comece com o b√°sico (hoje). Chains, RAG e Agents v√™m nos pr√≥ximos dias.

### LangChain √© obrigat√≥rio?

**N√£o!** Mas √© altamente recomendado para projetos profissionais. C√≥digo manual funciona, mas LangChain facilita muito.

### Posso misturar c√≥digo manual com LangChain?

**Sim!** LangChain √© flex√≠vel. Voc√™ pode usar onde faz sentido.

### Qual LLM usar?

**Para come√ßar:** Groq (gratuito e r√°pido)  
**Para produ√ß√£o:** Depende do caso. Teste v√°rios e escolha o melhor.

---

**√öltima atualiza√ß√£o:** 1 Dez 2025  
**Pr√≥ximo:** Execute `exemplo_langchain_basico.py` e complete os exerc√≠cios!

